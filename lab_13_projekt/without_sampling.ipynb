{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, accuracy_score\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "extractor = pipeline(\"feature-extraction\", model=\"distilbert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "splits = 200\n",
    "\n",
    "def get_embedding(review):\n",
    "    tokens = review.split()\n",
    "    while len(tokenizer.tokenize(\" \".join(tokens))) > 510:\n",
    "        tokens = tokens[1:]\n",
    "    tokens = \" \".join(tokens)\n",
    "    return extractor.transform(tokens)[0][-1]\n",
    "\n",
    "def make_embedding(df, add_rating):\n",
    "    dfs = []\n",
    "    for id, indices in enumerate(np.array_split(df.index.values, splits)):\n",
    "        temp = df.loc[indices]\n",
    "        temp[\"embedding\"] = temp.review.apply(get_embedding)\n",
    "        out_df = pd.DataFrame(torch.tensor(temp.embedding.values.tolist()), index=temp.index)\n",
    "        if add_rating:\n",
    "            out_df[\"target\"] = df[\"rating\"]\n",
    "        dfs.append(out_df)\n",
    "        print(str(id + 1) + \" out of: \" + str(splits))\n",
    "    output = pd.concat(dfs, axis=0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "test_data = pd.read_csv(\"test_data.csv\", header=None)\n",
    "test_data.columns = [\"review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  review  rating\n0      location not palace excellent hotel booke dthe...       4\n1      respite definitely not place stay looking ultr...       3\n2      stunning truly memorable spot right beach nusa...       4\n3      solid business hotel near embassy stayed hotel...       3\n4      nice place make sure lock money warning money ...       3\n...                                                  ...     ...\n16387  great base explore new york stayed 4 nights en...       4\n16388  wonderful advert paris wonderful introduction ...       4\n16389  ideal relaxing holdiay rachel jay green liverp...       3\n16390  watch food, husband went resort 4 nights chris...       2\n16391  fantastic hotel central barcelona family just ...       4\n\n[16392 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>location not palace excellent hotel booke dthe...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>respite definitely not place stay looking ultr...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>stunning truly memorable spot right beach nusa...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>solid business hotel near embassy stayed hotel...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>nice place make sure lock money warning money ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16387</th>\n      <td>great base explore new york stayed 4 nights en...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>16388</th>\n      <td>wonderful advert paris wonderful introduction ...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>16389</th>\n      <td>ideal relaxing holdiay rachel jay green liverp...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>16390</th>\n      <td>watch food, husband went resort 4 nights chris...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>16391</th>\n      <td>fantastic hotel central barcelona family just ...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>16392 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 review\n0     great hotel location stayed 4 nts 24th 28th ja...\n1     n't return overall disappointed hotel, no hot ...\n2     great value location desired problem hotel loc...\n3     kind helpfull people people kind helpful.we no...\n4     absolutely fabulous melia comfortable star hot...\n...                                                 ...\n4094  cockroaches dirty carpeting not consider 10 de...\n4095  ca n't wait return, husband stayed el san juan...\n4096  coming home stay wind chimes inn like coming h...\n4097  good hotel great location stayed apsis splendi...\n4098  major ripoff, ripoff, place dump capital d. ar...\n\n[4099 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>great hotel location stayed 4 nts 24th 28th ja...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>n't return overall disappointed hotel, no hot ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>great value location desired problem hotel loc...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>kind helpfull people people kind helpful.we no...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>absolutely fabulous melia comfortable star hot...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4094</th>\n      <td>cockroaches dirty carpeting not consider 10 de...</td>\n    </tr>\n    <tr>\n      <th>4095</th>\n      <td>ca n't wait return, husband stayed el san juan...</td>\n    </tr>\n    <tr>\n      <th>4096</th>\n      <td>coming home stay wind chimes inn like coming h...</td>\n    </tr>\n    <tr>\n      <th>4097</th>\n      <td>good hotel great location stayed apsis splendi...</td>\n    </tr>\n    <tr>\n      <th>4098</th>\n      <td>major ripoff, ripoff, place dump capital d. ar...</td>\n    </tr>\n  </tbody>\n</table>\n<p>4099 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#train_data_embedded = make_embedding(train_data, True)\n",
    "#train_data_embedded.to_csv(os.path.join(os.getcwd(), \"train_embedded.csv\"),)\n",
    "train_data_embedded = pd.read_csv(\"train_embedded.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#test_data_embedded = make_embedding(test_data, False)\n",
    "#test_data_embedded.to_csv(os.path.join(os.getcwd(), \"test_embedded.csv\"),)\n",
    "test_data_embedded = pd.read_csv(\"test_embedded.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              0         1         2         3         4         5         6  \\\n0      0.731024  0.250655 -0.089744  0.520189 -0.351825 -0.796953  0.368891   \n1      0.852949  0.236835  0.063832  0.469964 -0.363206 -1.045099  0.504673   \n2      0.685062  0.223866 -0.147828  0.606955 -0.273633 -1.010276  0.427389   \n3      0.604577  0.156163  0.297397  0.451490 -0.491574 -0.985112  0.442942   \n4      0.880773  0.142636 -0.071651  0.651384 -0.365893 -1.035383  0.384419   \n...         ...       ...       ...       ...       ...       ...       ...   \n16387  0.835931  0.129018  0.022031  0.611350 -0.468871 -0.967782  0.495816   \n16388  0.940912  0.275095  0.001012  0.571690 -0.366752 -0.934709  0.502089   \n16389  0.777620  0.096441 -0.056846  0.591733 -0.312893 -1.033518  0.436032   \n16390  0.732554  0.293466 -0.137129  0.726583 -0.281647 -0.734680  0.368102   \n16391  0.843220  0.112645  0.103516  0.465720 -0.388543 -0.976567  0.513763   \n\n              7         8         9  ...       759       760       761  \\\n0     -0.485046  0.544405  0.012710  ... -0.248937 -0.329662 -1.336018   \n1     -0.323598  0.351677 -0.058628  ... -0.053854 -0.379581 -1.176143   \n2     -0.193593  0.612398 -0.289408  ... -0.314359 -0.234852 -0.970357   \n3     -0.197288  0.268713 -0.115580  ... -0.330194 -0.097302 -0.903164   \n4     -0.315585  0.555099 -0.049492  ... -0.309621 -0.255526 -0.991006   \n...         ...       ...       ...  ...       ...       ...       ...   \n16387 -0.377074  0.321901 -0.078694  ... -0.251063 -0.303376 -1.200032   \n16388 -0.325906  0.531427 -0.064609  ... -0.124844 -0.253585 -1.128829   \n16389 -0.263038  0.386873  0.010131  ... -0.326421 -0.174483 -1.163905   \n16390 -0.346282  0.417760 -0.047382  ... -0.211326 -0.092348 -1.141169   \n16391 -0.066758  0.298830  0.033704  ... -0.135310 -0.055160 -0.982242   \n\n            762       763       764       765       766       767  target  \n0      0.294242 -1.056268 -0.317274 -0.079945 -0.374467 -0.185057       4  \n1      0.283369 -0.969518 -0.676686 -0.048067 -0.012696 -0.413625       3  \n2      0.102805 -0.914143 -0.388480 -0.169932 -0.066460 -0.435708       4  \n3      0.193958 -0.756460 -0.614573 -0.158738  0.176252 -0.584963       3  \n4      0.148594 -0.816133 -0.232153 -0.065063 -0.188306 -0.234032       3  \n...         ...       ...       ...       ...       ...       ...     ...  \n16387  0.222648 -0.893699 -0.576341 -0.013637 -0.038671 -0.373459       4  \n16388  0.364104 -0.857907 -0.352951 -0.022036 -0.106580 -0.261812       4  \n16389  0.222434 -0.881252 -0.462759  0.088253  0.021210 -0.329125       3  \n16390  0.191584 -0.932992 -0.262553  0.038973 -0.071927 -0.228940       2  \n16391  0.222882 -0.829017 -0.597377 -0.030557 -0.085931 -0.452981       4  \n\n[16392 rows x 769 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.731024</td>\n      <td>0.250655</td>\n      <td>-0.089744</td>\n      <td>0.520189</td>\n      <td>-0.351825</td>\n      <td>-0.796953</td>\n      <td>0.368891</td>\n      <td>-0.485046</td>\n      <td>0.544405</td>\n      <td>0.012710</td>\n      <td>...</td>\n      <td>-0.248937</td>\n      <td>-0.329662</td>\n      <td>-1.336018</td>\n      <td>0.294242</td>\n      <td>-1.056268</td>\n      <td>-0.317274</td>\n      <td>-0.079945</td>\n      <td>-0.374467</td>\n      <td>-0.185057</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.852949</td>\n      <td>0.236835</td>\n      <td>0.063832</td>\n      <td>0.469964</td>\n      <td>-0.363206</td>\n      <td>-1.045099</td>\n      <td>0.504673</td>\n      <td>-0.323598</td>\n      <td>0.351677</td>\n      <td>-0.058628</td>\n      <td>...</td>\n      <td>-0.053854</td>\n      <td>-0.379581</td>\n      <td>-1.176143</td>\n      <td>0.283369</td>\n      <td>-0.969518</td>\n      <td>-0.676686</td>\n      <td>-0.048067</td>\n      <td>-0.012696</td>\n      <td>-0.413625</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.685062</td>\n      <td>0.223866</td>\n      <td>-0.147828</td>\n      <td>0.606955</td>\n      <td>-0.273633</td>\n      <td>-1.010276</td>\n      <td>0.427389</td>\n      <td>-0.193593</td>\n      <td>0.612398</td>\n      <td>-0.289408</td>\n      <td>...</td>\n      <td>-0.314359</td>\n      <td>-0.234852</td>\n      <td>-0.970357</td>\n      <td>0.102805</td>\n      <td>-0.914143</td>\n      <td>-0.388480</td>\n      <td>-0.169932</td>\n      <td>-0.066460</td>\n      <td>-0.435708</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.604577</td>\n      <td>0.156163</td>\n      <td>0.297397</td>\n      <td>0.451490</td>\n      <td>-0.491574</td>\n      <td>-0.985112</td>\n      <td>0.442942</td>\n      <td>-0.197288</td>\n      <td>0.268713</td>\n      <td>-0.115580</td>\n      <td>...</td>\n      <td>-0.330194</td>\n      <td>-0.097302</td>\n      <td>-0.903164</td>\n      <td>0.193958</td>\n      <td>-0.756460</td>\n      <td>-0.614573</td>\n      <td>-0.158738</td>\n      <td>0.176252</td>\n      <td>-0.584963</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.880773</td>\n      <td>0.142636</td>\n      <td>-0.071651</td>\n      <td>0.651384</td>\n      <td>-0.365893</td>\n      <td>-1.035383</td>\n      <td>0.384419</td>\n      <td>-0.315585</td>\n      <td>0.555099</td>\n      <td>-0.049492</td>\n      <td>...</td>\n      <td>-0.309621</td>\n      <td>-0.255526</td>\n      <td>-0.991006</td>\n      <td>0.148594</td>\n      <td>-0.816133</td>\n      <td>-0.232153</td>\n      <td>-0.065063</td>\n      <td>-0.188306</td>\n      <td>-0.234032</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16387</th>\n      <td>0.835931</td>\n      <td>0.129018</td>\n      <td>0.022031</td>\n      <td>0.611350</td>\n      <td>-0.468871</td>\n      <td>-0.967782</td>\n      <td>0.495816</td>\n      <td>-0.377074</td>\n      <td>0.321901</td>\n      <td>-0.078694</td>\n      <td>...</td>\n      <td>-0.251063</td>\n      <td>-0.303376</td>\n      <td>-1.200032</td>\n      <td>0.222648</td>\n      <td>-0.893699</td>\n      <td>-0.576341</td>\n      <td>-0.013637</td>\n      <td>-0.038671</td>\n      <td>-0.373459</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>16388</th>\n      <td>0.940912</td>\n      <td>0.275095</td>\n      <td>0.001012</td>\n      <td>0.571690</td>\n      <td>-0.366752</td>\n      <td>-0.934709</td>\n      <td>0.502089</td>\n      <td>-0.325906</td>\n      <td>0.531427</td>\n      <td>-0.064609</td>\n      <td>...</td>\n      <td>-0.124844</td>\n      <td>-0.253585</td>\n      <td>-1.128829</td>\n      <td>0.364104</td>\n      <td>-0.857907</td>\n      <td>-0.352951</td>\n      <td>-0.022036</td>\n      <td>-0.106580</td>\n      <td>-0.261812</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>16389</th>\n      <td>0.777620</td>\n      <td>0.096441</td>\n      <td>-0.056846</td>\n      <td>0.591733</td>\n      <td>-0.312893</td>\n      <td>-1.033518</td>\n      <td>0.436032</td>\n      <td>-0.263038</td>\n      <td>0.386873</td>\n      <td>0.010131</td>\n      <td>...</td>\n      <td>-0.326421</td>\n      <td>-0.174483</td>\n      <td>-1.163905</td>\n      <td>0.222434</td>\n      <td>-0.881252</td>\n      <td>-0.462759</td>\n      <td>0.088253</td>\n      <td>0.021210</td>\n      <td>-0.329125</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>16390</th>\n      <td>0.732554</td>\n      <td>0.293466</td>\n      <td>-0.137129</td>\n      <td>0.726583</td>\n      <td>-0.281647</td>\n      <td>-0.734680</td>\n      <td>0.368102</td>\n      <td>-0.346282</td>\n      <td>0.417760</td>\n      <td>-0.047382</td>\n      <td>...</td>\n      <td>-0.211326</td>\n      <td>-0.092348</td>\n      <td>-1.141169</td>\n      <td>0.191584</td>\n      <td>-0.932992</td>\n      <td>-0.262553</td>\n      <td>0.038973</td>\n      <td>-0.071927</td>\n      <td>-0.228940</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>16391</th>\n      <td>0.843220</td>\n      <td>0.112645</td>\n      <td>0.103516</td>\n      <td>0.465720</td>\n      <td>-0.388543</td>\n      <td>-0.976567</td>\n      <td>0.513763</td>\n      <td>-0.066758</td>\n      <td>0.298830</td>\n      <td>0.033704</td>\n      <td>...</td>\n      <td>-0.135310</td>\n      <td>-0.055160</td>\n      <td>-0.982242</td>\n      <td>0.222882</td>\n      <td>-0.829017</td>\n      <td>-0.597377</td>\n      <td>-0.030557</td>\n      <td>-0.085931</td>\n      <td>-0.452981</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>16392 rows × 769 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             0         1         2         3         4         5         6  \\\n0     0.938493  0.183004  0.063817  0.606710 -0.451647 -1.035825  0.497661   \n1     0.706570  0.297040  0.028349  0.577732 -0.434663 -0.822771  0.561691   \n2     0.641337  0.111375 -0.158317  0.652595 -0.282199 -0.709899  0.479667   \n3     0.795235  0.288830  0.081681  0.478825 -0.349272 -0.951365  0.739767   \n4     0.878483  0.168300  0.053744  0.619344 -0.258921 -0.971106  0.514012   \n...        ...       ...       ...       ...       ...       ...       ...   \n4094  0.691393  0.205510  0.025593  0.584929 -0.429532 -0.889108  0.373620   \n4095  0.785465  0.201247  0.059720  0.534150 -0.293193 -1.000153  0.477972   \n4096  0.778980  0.311188 -0.053091  0.542757 -0.259970 -1.140321  0.566421   \n4097  0.903361  0.103976 -0.061863  0.502919 -0.379310 -0.841914  0.578198   \n4098  0.702961  0.133766 -0.159329  0.600802 -0.262643 -0.886324  0.613870   \n\n             7         8         9  ...       758       759       760  \\\n0    -0.335741  0.296220 -0.089404  ...  0.320153 -0.215700 -0.141498   \n1    -0.450103  0.477983  0.191853  ...  0.391724 -0.174689 -0.279862   \n2    -0.350484  0.658158 -0.103099  ...  0.460593 -0.204626 -0.182919   \n3     0.119375  0.262919 -0.324973  ...  0.511579 -0.089489 -0.287653   \n4    -0.315954  0.336959 -0.046220  ...  0.448769 -0.251376 -0.189057   \n...        ...       ...       ...  ...       ...       ...       ...   \n4094 -0.327372  0.362951 -0.157241  ...  0.571003 -0.216551 -0.152186   \n4095 -0.359672  0.261462  0.010799  ...  0.269770 -0.172465 -0.264479   \n4096 -0.276461  0.482846 -0.077032  ...  0.432233 -0.255068 -0.210363   \n4097 -0.225339  0.459133 -0.083272  ...  0.476695 -0.134333 -0.282957   \n4098 -0.327964  0.476557 -0.104983  ...  0.498087 -0.149090 -0.264274   \n\n           761       762       763       764       765       766       767  \n0    -1.083900  0.211511 -0.850142 -0.530651 -0.013141 -0.012572 -0.348831  \n1    -1.175929  0.155675 -1.090621 -0.480290 -0.028343 -0.080079 -0.208822  \n2    -1.175709  0.084220 -1.226739 -0.539408 -0.012542 -0.196584 -0.291981  \n3    -0.946852  0.157680 -0.753880 -0.655168 -0.502852  0.179739 -0.493145  \n4    -1.224119  0.270882 -0.938721 -0.318829 -0.003921 -0.071376 -0.336138  \n...        ...       ...       ...       ...       ...       ...       ...  \n4094 -1.159467  0.071518 -0.893078 -0.720746 -0.094627  0.159722 -0.414767  \n4095 -1.016061  0.078172 -0.816504 -0.544462 -0.045216 -0.075585 -0.418826  \n4096 -1.114083  0.263386 -0.951518 -0.053777 -0.038128 -0.146547 -0.352711  \n4097 -0.949641  0.156362 -0.719047 -0.611670  0.055877 -0.075826 -0.399337  \n4098 -1.227384  0.029183 -0.934078 -0.730961 -0.193486 -0.065459 -0.311859  \n\n[4099 rows x 768 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>758</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.938493</td>\n      <td>0.183004</td>\n      <td>0.063817</td>\n      <td>0.606710</td>\n      <td>-0.451647</td>\n      <td>-1.035825</td>\n      <td>0.497661</td>\n      <td>-0.335741</td>\n      <td>0.296220</td>\n      <td>-0.089404</td>\n      <td>...</td>\n      <td>0.320153</td>\n      <td>-0.215700</td>\n      <td>-0.141498</td>\n      <td>-1.083900</td>\n      <td>0.211511</td>\n      <td>-0.850142</td>\n      <td>-0.530651</td>\n      <td>-0.013141</td>\n      <td>-0.012572</td>\n      <td>-0.348831</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.706570</td>\n      <td>0.297040</td>\n      <td>0.028349</td>\n      <td>0.577732</td>\n      <td>-0.434663</td>\n      <td>-0.822771</td>\n      <td>0.561691</td>\n      <td>-0.450103</td>\n      <td>0.477983</td>\n      <td>0.191853</td>\n      <td>...</td>\n      <td>0.391724</td>\n      <td>-0.174689</td>\n      <td>-0.279862</td>\n      <td>-1.175929</td>\n      <td>0.155675</td>\n      <td>-1.090621</td>\n      <td>-0.480290</td>\n      <td>-0.028343</td>\n      <td>-0.080079</td>\n      <td>-0.208822</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.641337</td>\n      <td>0.111375</td>\n      <td>-0.158317</td>\n      <td>0.652595</td>\n      <td>-0.282199</td>\n      <td>-0.709899</td>\n      <td>0.479667</td>\n      <td>-0.350484</td>\n      <td>0.658158</td>\n      <td>-0.103099</td>\n      <td>...</td>\n      <td>0.460593</td>\n      <td>-0.204626</td>\n      <td>-0.182919</td>\n      <td>-1.175709</td>\n      <td>0.084220</td>\n      <td>-1.226739</td>\n      <td>-0.539408</td>\n      <td>-0.012542</td>\n      <td>-0.196584</td>\n      <td>-0.291981</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.795235</td>\n      <td>0.288830</td>\n      <td>0.081681</td>\n      <td>0.478825</td>\n      <td>-0.349272</td>\n      <td>-0.951365</td>\n      <td>0.739767</td>\n      <td>0.119375</td>\n      <td>0.262919</td>\n      <td>-0.324973</td>\n      <td>...</td>\n      <td>0.511579</td>\n      <td>-0.089489</td>\n      <td>-0.287653</td>\n      <td>-0.946852</td>\n      <td>0.157680</td>\n      <td>-0.753880</td>\n      <td>-0.655168</td>\n      <td>-0.502852</td>\n      <td>0.179739</td>\n      <td>-0.493145</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.878483</td>\n      <td>0.168300</td>\n      <td>0.053744</td>\n      <td>0.619344</td>\n      <td>-0.258921</td>\n      <td>-0.971106</td>\n      <td>0.514012</td>\n      <td>-0.315954</td>\n      <td>0.336959</td>\n      <td>-0.046220</td>\n      <td>...</td>\n      <td>0.448769</td>\n      <td>-0.251376</td>\n      <td>-0.189057</td>\n      <td>-1.224119</td>\n      <td>0.270882</td>\n      <td>-0.938721</td>\n      <td>-0.318829</td>\n      <td>-0.003921</td>\n      <td>-0.071376</td>\n      <td>-0.336138</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4094</th>\n      <td>0.691393</td>\n      <td>0.205510</td>\n      <td>0.025593</td>\n      <td>0.584929</td>\n      <td>-0.429532</td>\n      <td>-0.889108</td>\n      <td>0.373620</td>\n      <td>-0.327372</td>\n      <td>0.362951</td>\n      <td>-0.157241</td>\n      <td>...</td>\n      <td>0.571003</td>\n      <td>-0.216551</td>\n      <td>-0.152186</td>\n      <td>-1.159467</td>\n      <td>0.071518</td>\n      <td>-0.893078</td>\n      <td>-0.720746</td>\n      <td>-0.094627</td>\n      <td>0.159722</td>\n      <td>-0.414767</td>\n    </tr>\n    <tr>\n      <th>4095</th>\n      <td>0.785465</td>\n      <td>0.201247</td>\n      <td>0.059720</td>\n      <td>0.534150</td>\n      <td>-0.293193</td>\n      <td>-1.000153</td>\n      <td>0.477972</td>\n      <td>-0.359672</td>\n      <td>0.261462</td>\n      <td>0.010799</td>\n      <td>...</td>\n      <td>0.269770</td>\n      <td>-0.172465</td>\n      <td>-0.264479</td>\n      <td>-1.016061</td>\n      <td>0.078172</td>\n      <td>-0.816504</td>\n      <td>-0.544462</td>\n      <td>-0.045216</td>\n      <td>-0.075585</td>\n      <td>-0.418826</td>\n    </tr>\n    <tr>\n      <th>4096</th>\n      <td>0.778980</td>\n      <td>0.311188</td>\n      <td>-0.053091</td>\n      <td>0.542757</td>\n      <td>-0.259970</td>\n      <td>-1.140321</td>\n      <td>0.566421</td>\n      <td>-0.276461</td>\n      <td>0.482846</td>\n      <td>-0.077032</td>\n      <td>...</td>\n      <td>0.432233</td>\n      <td>-0.255068</td>\n      <td>-0.210363</td>\n      <td>-1.114083</td>\n      <td>0.263386</td>\n      <td>-0.951518</td>\n      <td>-0.053777</td>\n      <td>-0.038128</td>\n      <td>-0.146547</td>\n      <td>-0.352711</td>\n    </tr>\n    <tr>\n      <th>4097</th>\n      <td>0.903361</td>\n      <td>0.103976</td>\n      <td>-0.061863</td>\n      <td>0.502919</td>\n      <td>-0.379310</td>\n      <td>-0.841914</td>\n      <td>0.578198</td>\n      <td>-0.225339</td>\n      <td>0.459133</td>\n      <td>-0.083272</td>\n      <td>...</td>\n      <td>0.476695</td>\n      <td>-0.134333</td>\n      <td>-0.282957</td>\n      <td>-0.949641</td>\n      <td>0.156362</td>\n      <td>-0.719047</td>\n      <td>-0.611670</td>\n      <td>0.055877</td>\n      <td>-0.075826</td>\n      <td>-0.399337</td>\n    </tr>\n    <tr>\n      <th>4098</th>\n      <td>0.702961</td>\n      <td>0.133766</td>\n      <td>-0.159329</td>\n      <td>0.600802</td>\n      <td>-0.262643</td>\n      <td>-0.886324</td>\n      <td>0.613870</td>\n      <td>-0.327964</td>\n      <td>0.476557</td>\n      <td>-0.104983</td>\n      <td>...</td>\n      <td>0.498087</td>\n      <td>-0.149090</td>\n      <td>-0.264274</td>\n      <td>-1.227384</td>\n      <td>0.029183</td>\n      <td>-0.934078</td>\n      <td>-0.730961</td>\n      <td>-0.193486</td>\n      <td>-0.065459</td>\n      <td>-0.311859</td>\n    </tr>\n  </tbody>\n</table>\n<p>4099 rows × 768 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "complete_set = train_data_embedded\n",
    "X = complete_set.loc[:, complete_set.columns != 'target']\n",
    "y = complete_set['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('standardscaler', StandardScaler()),\n                ('svc', SVC(class_weight='balanced'))])",
      "text/html": "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n                (&#x27;svc&#x27;, SVC(class_weight=&#x27;balanced&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n                (&#x27;svc&#x27;, SVC(class_weight=&#x27;balanced&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div></div></div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model = make_pipeline(StandardScaler(), SVC(class_weight='balanced'))\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n",
      " balanced_accuracy = 0.8071860558243801, accuracy = 0.7414016624723557\n",
      " f1_score: 0.7430589646134262\n",
      "TEST: \n",
      " balanced_accuracy = 0.5098842515073029, accuracy = 0.5468130527599878\n",
      " f1_score: 0.5576391365979276\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(f\"TRAIN:\\n balanced_accuracy = {balanced_accuracy_score(y_train, y_pred_train)}, accuracy = {accuracy_score(y_train, y_pred_train)}\\n f1_score: {f1_score(y_train, y_pred_train, average='weighted')}\")\n",
    "print(f\"TEST: \\n balanced_accuracy = {balanced_accuracy_score(y_test, y_pred_test)}, accuracy = {accuracy_score(y_test, y_pred_test)}\\n f1_score: {f1_score(y_test, y_pred_test, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data_embedded)\n",
    "pd.Series(predictions).to_csv(\"piatek_Brus_Maj.csv\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}